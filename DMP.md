# Data management Plan (DMP)
## Abstract: Implementing a Data Management Plan for Enhanced Research
### Background
The increasing emphasis on Open Science within the research landscape necessitates robust data management practices to ensure that research data is **FAIR** (**F**indable, **A**ccessible, **I**nteroperable, **R**eusable). Data Management Plans (DMPs) play a crucial role in achieving this goal by providing a structured framework for researchers to consider and plan for all aspects of data management throughout the research lifecycle. Krause et al. and Science Europe 2021 provide valuable insights and guidelines for developing and implementing effective DMPs.
### Results
Implementing a DMP, guided by the principles and criteria outlined in Krause et al. and Science Europe 2021 and utilising the SODAR platform, could potentially lead to: 
*   **Structured Metadata Management**: SODAR utilises the ISA-Tab format, ensuring standardised and structured metadata collection. This facilitates efficient data organisation and retrieval. Additionally, the platform offers templates and upload capabilities for ISA-Tab files, streamlining the metadata creation process. The ability to edit sample sheets with features like ontology term lookup and version control further enhances metadata quality and accessibility.
*   **Robust Data Storage and Access**: SODAR leverages a distributed file system based on iRODS, providing a secure and scalable solution for storing large-scale datasets. Landing zones within SODAR facilitate controlled data uploads and ensure file integrity through checksum validation. Furthermore, SODAR offers diverse data access methods, including iRODS commands, CUBI Toolkit integration, and WebDAV mounting, accommodating varied user needs.
*   **Streamlined Data Pipelines**: Your existing data pipelines can be seamlessly integrated with SODAR, enabling efficient data flow and analysis. The platform supports automated shortcut generation for commonly used file types and integrates with tools like IGV and UCSC Genome Browser.
*   **Enhanced Collaboration and Data Sharing**: SODAR facilitates collaborative research through project-based access controls and data encapsulation. It also provides tools for generating iRODS access tickets, enabling controlled data sharing with external collaborators or third-party applications.
*   **Improved Data Quality**: SODAR's emphasis on data validation during the upload process helps ensure data integrity and quality. Its integration with data analysis and visualisation tools further supports quality control efforts throughout the research process.
*   **SODAR and External Public Repositories**: While SODAR does not currently offer a generic exporter for public databases due to its flexible metadata model, exporting is feasible if the project is set up to be compatible with the target repository. In the future, SODAR aims to enable direct export to repositories like NFDI's GHGA, which could serve as a model for exporting to other platforms. 
*   **The "Modellvorhaben Genomsequenzierung nach § 64e SGB V"** is a German government initiative that aims to enhance the diagnosis and treatment of both rare and oncological diseases by integrating comprehensive genome sequencing into a structured clinical workflow. This model project seeks to combine clinical and genomic data within a dedicated infrastructure to facilitate advanced analysis. The goal is to improve medical understanding and treatment strategies for the targeted diseases.  Although SODAR does not currently support this project, the Core Unit Bioinformatics (CUBI) is working to integrate it, enabling researchers to leverage SODAR for managing the vast amounts of genomic and clinical data generated by the project. 

### Conclusion
The development and implementation of a well-structured Data Management Plan (DMP), guided by best practices from Krause et al. and Science Europe 2021, alongside the SODAR platform’s capabilities, represents a significant step forward in achieving FAIR data principles in research. Through its comprehensive features—spanning metadata standardization, secure data storage, streamlined data workflows, and collaboration tools—SODAR addresses key challenges in data management, promoting data accessibility, quality, and reusability. This framework fosters a collaborative and transparent research environment that not only enhances the reproducibility and integrity of scientific findings but also paves the way for more innovative and impactful discoveries across disciplines.

### References
1. Krause, C., Bergmann, K., Hausen, D. A., Riedel, R., & Windeck, J. (2024). Quality Criteria for DMP Templates (1.1). Zenodo. https://doi.org/10.5281/zenodo.13347687
2. Science Europe. (2021). Practical Guide to the International Alignment of Research Data Management - Extended Edition. https://doi.org/10.5281/zenodo.4915862
3. Nieminen, Mikko, et al. "SODAR: managing multiomics study data and metadata." GigaScience 12 (2023): giad052.

## Advantages of Using Text Modules in DMP Templates

**Text modules** in DMP templates provide pre-written answers that users can select or adapt, making it easier and faster for them to complete their DMPs. This is especially helpful for questions that require detailed answers, like questions about long-term archiving or intellectual property rights. 

Here are some specific advantages:

*   **Simplifying the DMP completion process**: Text modules help users complete their DMPs more quickly and easily. Instead of writing detailed responses from scratch, they can choose from a set of pre-written options or adapt existing ones to their specific needs. This is particularly helpful for users who may be new to data management planning or unsure how to answer certain questions.
*   **Improving consistency and accuracy**: Text modules can ensure that certain information is presented consistently and accurately across different DMPs. This is helpful for institutions or funders who want to ensure that certain standards are met or that key information is included in all DMPs.
*   **Facilitating automated evaluation**: Text modules can be designed to facilitate automated evaluation of DMPs. For example, if the text modules are structured and standardised, it becomes easier for software to analyse and extract key information from the DMPs. 
*   **Supporting users with limited experience**: Text modules can be particularly helpful for users with limited experience in data management planning. By providing pre-written guidance and examples, text modules can help these users understand the requirements of a DMP and how to address them effectively.

Science Europe also point out that text modules are not always appropriate or necessary. They should be used judiciously, focusing on areas where they can provide the most benefit to users.

## Benefits of the machine-actionable DMP (maDMP) Application Profile

Adopting the application profile for maDMPs brings several benefits:

*   **Machine-actionability:** maDMP promotes the use of **structured information** and **closed questions**. This allows computers to process and analyse DMPs, improving data management practices and **automating tasks** such as data validation and transfer. For example, systems can use this structured data to **automatically fill out information** for data deposited into repositories. 
*   **Comprehensive Coverage**: maDMP encourages addressing a wide range of data management aspects in questionnaires, including IT, archival, data publication, sustainability, and FAIR principles. This ensures researchers consider all relevant aspects, even those they might not initially anticipate.
*   **Simplified DMP Creation and Reporting**: maDMP facilitates the creation of **high-quality DMPs** by using a **question-and-answer format** with pre-defined answers and conditional follow-up questions. This approach guides researchers and allows for automatic generation of DMP reports in various formats.
*   **Automated DMP Evaluation**:  maDMP supports the development of metrics for automatically evaluating DMPs against FAIR and Open Science principles. These metrics provide researchers and funders with quantitative insights into data FAIRness and Openness, enabling improvements in data management practices.

Overall, the application profile for maDMPs helps to create a more efficient, effective, and interoperable data management ecosystem. This leads to improved data quality, enhanced collaboration, and ultimately, better research outcomes. 

**Reference**

Miksa, Tomasz, et al. "Application Profile for Machine-Actionable Data Management Plans." (2021). [Link](https://datascience.codata.org/articles/10.5334/dsj-2021-032)

### Benefits of maDMP Application Profile in SODAR

Adopting the application profile for machine-actionable DMPs (maDMPs) within the SODAR framework, particularly when used with a data management wizard like the Data Stewardship Wizard (DSW), offers several key benefits. These advantages stem from the synergy between the structured, standardised nature of maDMPs and SODAR's capabilities in managing omics data and metadata. 

**Key Benefits:**

*   **Streamlined Data Management Workflow:**  The maDMP application profile, when implemented in a data management wizard like DSW, simplifies the process of creating and maintaining DMPs. DSW's hierarchical questionnaire format with closed questions and conditional follow-up questions guides researchers through the DMP creation process, ensuring all necessary information is captured in a structured and machine-readable manner. This structured data can then be seamlessly integrated with SODAR, which uses the ISA-Tab format for managing study design metadata. SODAR provides a GUI and APIs for interacting with this metadata, further streamlining data management activities.

*   **Automated Data Validation and Transfer:** SODAR, in conjunction with iRODS, provides robust data storage and management capabilities.  maDMPs created using a tool like DSW can define the expected data structure and format, enabling automated validation of data uploads through SODAR's landing zones. This automation minimizes the risk of errors and ensures data quality and consistency. Once validated, SODAR automatically transfers data to the appropriate location within the project's read-only sample repository in iRODS, simplifying data archival and access for project members.

*   **Enhanced Data Discoverability and Interoperability:** The maDMP application profile advocates for using controlled vocabularies and ontologies. DSW supports integrating external resources like FAIRsharing.org for this purpose, ensuring that researchers use standardised terminology in their DMPs. SODAR further enhances this by allowing the upload and use of ontologies for annotating sample sheets,  making data more discoverable and interoperable within and across research projects.

*   **Improved Data Analysis and Integration:**  SODAR offers tools and APIs to aid data analysis and integration. maDMPs can provide crucial information about the data's structure, format, and relationships, allowing bioinformaticians to easily access and analyse data within SODAR. Additionally, the integration of SODAR with tools like the IGV Genome Browser facilitates the visualisation and exploration of genomic data, demonstrating the potential of maDMPs to drive more efficient and effective data analysis workflows.

**Implementing maDMP Application Profile in SODAR with DSW:**

Pergl et al. explain that the **Data Stewardship Wizard (DSW)** is implemented as a web-based questionnaire tool driven by an expert system called the Knowledge Model. [YouTube Video](https://youtu.be/gcSPG_dyVUQ?si=h41Z2nauYYKRg7r1)

*   **Web Questionnaire Tool:** The DSW utilizes a web-based tool developed at the Czech Technical University in Prague. This tool presents users with hierarchical data management questionnaires and stores intermediate results in a database. 

*   **Knowledge Model:** The heart of the DSW is the Knowledge Model, which acts as an expert system. The Knowledge Model was initially derived from a mind map capturing years of experience in the life science domain. It consists of nested questions, possible answers, expert guidance, and links to external resources. 

    *   The mind map's content was translated into a structured **standard Knowledge Model** containing hundreds of questions organized into chapters that mirror the research data lifecycle.
    *   Data stewards can customize the standard Knowledge Model to fit their specific disciplines or institutions. 
    *   The Knowledge Model employs various question types:
        *   **Options:** Questions with a list of possible answers, each potentially leading to follow-up questions, creating a hierarchical tree structure. 
        *   **List of items:** Allows researchers to input multiple items, each with potential follow-up questions.
        *   **Open questions:** Questions that allow researchers to provide free-text answers.
    *   The Knowledge Model also incorporates guidance information for each question, including explanatory text, contact information for relevant experts, and links to external resources, including references to the book "Data Stewardship for Open Science". 
    *   An **Editor** allows data stewards to modify and customize the Knowledge Model, adding, removing, or modifying questions and guidance. 

*   **DMP Assembly:** One of the key functions of the DSW is to assemble the answers provided in the questionnaire into a **data management plan (DMP)**.

    *   The DSW generates textual DMPs from the questionnaire using encoded DMP templates in various formats, including HTML, PDF, MS Word, and LaTeX. 
    *   The system utilizes the Jinja2 template language for customization and can generate concise DMPs targeting specific funder requirements, such as those based on Science Europe recommendations. 

*   **Automated DMP Evaluation:** The DSW incorporates metrics to evaluate DMPs against **FAIR** (Findable, Accessible, Interoperable, Reusable) and Open Science principles. 

    *   These metrics are linked to the closed questions in the Knowledge Model and provide insights into how well the DMP adheres to these principles. 
    *   The DSW generates reports summarizing these metrics, enabling researchers and funders to assess the potential FAIRness and Openness of the data based on the DMP. 

*   **Availability and Collaboration:**

    *   The DSW is an open-source project, with code and documentation publicly available. It can be installed on-site or through Docker containers. 
    *   The project encourages contributions to both the code and the Knowledge Model from the data stewardship community. 

Pergl et al. highlight that the DSW is a tool built to facilitate better data management practices and promote the creation of **machine-actionable DMPs**. It achieves this through a structured approach that combines a web-based questionnaire tool with a sophisticated Knowledge Model, enabling the generation of high-quality, customizable DMPs that can be automatically evaluated against FAIR and Open Science principles. 

**Reference**

Pergl, Robert, et al. "“Data Stewardship Wizard”: A tool bringing together researchers, data stewards, and data experts around data management planning." Data Science Journal 18 (2019): 59-59.[Link1](https://datascience.codata.org/articles/10.5334/dsj-2019-059); [DSW](https://ds-wizard.org/get-started)

## Brief description of DMP criterias elaborated in Krause et al. and Science Europe 2021:
The document by Krause et al., "Quality Criteria for DMP Templates – Analysis across NFDI," presents a set of guidelines for creating high-quality Data Management Plans (DMPs). The authors, a group from the National Research Data Infrastructure (NFDI) in Germany, have analysed various DMP templates and identified a catalogue of quality criteria, classified into five levels of complexity. These criteria, which are both content-related and formal, cover essential elements such as core requirements, data quality, FAIR principles, and legal considerations. The document also provides a table of quality criteria, illustrating their application with examples from existing DMP templates, and a list of example catalogues for further reference. The ultimate goal of this work is to promote consistency and comparability of DMPs across the NFDI, facilitating easier identification and development of suitable templates for different research needs.

### Krause et al. outline two main categories of criteria used to evaluate Data Management Plan (DMP) templates:

**Content-Related Criteria**

This category focuses on the substantive aspects of the DMP template, ensuring that it addresses crucial areas of data management planning. Krause et al. provide several examples of content-related criteria, including:

*   **Core Requirements**:  The template should cover the fundamental requirements for DMPs as defined by organisations like Science Europe. These requirements generally address topics such as data description, documentation, storage, legal and ethical considerations, data sharing and preservation, and responsibilities for data management.
*   **General Explanations**: The template should offer clear and concise explanations for each question, supported by links, notes, and relevant resources. This helps researchers to understand the rationale behind the questions and provides guidance on how to answer them effectively.
*   **Question Cascade**:  The template should guide researchers through a logical progression of questions, moving from general to more specific aspects of data management. This structured approach facilitates a comprehensive and well-considered DMP.
*   **Data Quality**:  The template should address data quality considerations at different levels of complexity, from basic questions about quality inspection to more advanced guidance on implementing measures and strategies for ensuring data quality.
*   **FAIR Data Principles**: The template should promote the FAIR principles (Findability, Accessibility, Interoperability, and Reusability) by incorporating questions and explanations that encourage researchers to consider these principles in their data management planning.

**Formal Criteria**

This category focuses on the structure, presentation, and user-friendliness of the DMP template.  Examples of formal criteria mentioned in Krause et al. include:

*   **Distinctness**:  The template should have a clear and logical structure, with questions that are precisely formulated and avoid redundancy or overlap. This ensures that the template is easy to understand and navigate.
*   **Process Optimisation**: The template should align with the typical work processes and data life cycle stages relevant to the target research community or institution. This facilitates a more intuitive and efficient data management planning process.
*   **Answer Options**: Providing pre-defined answer options in the form of bullet points or checkboxes can simplify and expedite the process of completing the DMP, while also promoting consistency in responses.
*   **Text Modules**: Offering pre-written text modules for specific questions, particularly those requiring detailed responses, can be helpful for users who may be unfamiliar with certain aspects of data management planning. These modules can be adapted to the specific context of the research project.
*   **Modules**: A modular structure, with a core module containing essential information and optional, specialised modules that can be added as needed, offers flexibility and caters to diverse research needs.

By considering both content-related and formal criteria, DMP templates can be effectively evaluated to ensure their effectiveness, usability, and ability to support good data management practices.

### Science Europe's Core Requirements for DMPs
**This is a guide for research organisations and funding bodies on how to align their research data management (RDM) policies and practices across Europe. The guide aims to provide a common framework for managing research data that can be adapted to the needs of individual organisations and disciplines. It stresses that DMPs should not be a bureaucratic burden for researchers, but a helpful tool for planning and managing their research projects. The guide emphasises the importance of data sharing and preservation, and provides guidance on how to manage legal, ethical and security issues related to research data.**

Krause et al. identify **Science Europe's Core Requirements for DMPs** as a crucial Level 1 criterion for evaluating Data Management Plan (DMP) templates. This signifies its importance as a fundamental element that should be incorporated into generic DMP templates. 

**Six Core Requirements for Data Management Plans according to Science Europe**

Science Europe define **six core requirements** that should be addressed in every Data Management Plan (DMP). These requirements cover key aspects of data management and aim to provide a common basis for research funding organisations, research organisations, and research communities to develop their own RDM policies. The six requirements are:

1.  **Data description and collection or reuse of existing data**: This section requires researchers to describe how new data will be collected or produced, or how existing data will be reused. It includes questions about the methodologies, software, and constraints on reusing existing data.
2.  **Documentation and data quality**: Researchers need to outline what metadata and documentation will accompany the data, including information about the methodology of data collection and data organisation. This section also addresses data quality control measures used to ensure the consistency and quality of data.
3.  **Storage and backup during the research process**: This requirement focuses on how data and metadata will be stored and backed up during the research process. Researchers need to specify storage locations, backup frequencies, and data security measures, especially for sensitive data.
4.  **Legal and ethical requirements, codes of conduct**: This crucial section covers compliance with legal and ethical requirements, including legislation on personal data, data security, intellectual property rights, and relevant codes of conduct. Researchers must address issues of data ownership, access conditions, and ethical considerations related to data handling.
5.  **Data sharing and long-term preservation**: This requirement focuses on how and when data will be shared, including any restrictions or embargoes. Researchers must specify how data will be selected for preservation, where it will be preserved long-term (e.g., data repositories or archives), and what methods or tools will be needed to access and use the data. 
6.  **Data management responsibilities and resources**: The final requirement addresses the roles and responsibilities for data management, identifying the data steward and outlining the resources allocated for data management activities. This includes financial resources, staff time, and any costs associated with data preparation, deposit, and long-term preservation.

The order of these six core requirements can be modified based on specific needs and organisational priorities. However, it is crucial that **all six core requirements are addressed in a DMP** to ensure comprehensive data management planning. 

**Importance of Certification for Repositories**

The "Practical Guide" strongly recommends that repositories seek certification from an acknowledged certification body to enhance their trustworthiness and make it easier for researchers to identify reliable repositories for their data.

Science Europe explain that the research landscape is shifting towards increased **data sharing and open science practices**. As more research funding organisations, institutions, and journals require researchers to deposit their data in repositories, the number of available repositories has surged. Currently, over 2,000 repositories are listed in general registries, making it challenging for researchers to assess their trustworthiness. 

Certification plays a crucial role in addressing this challenge. When a repository obtains certification from an acknowledged body, it demonstrates its commitment to meeting specific standards and best practices for data management. This provides researchers with a reliable indicator of the repository's trustworthiness and helps them make informed decisions about where to deposit their valuable data. 

The "Practical Guide" suggests that researchers should prioritise **discipline-specific repositories or those that have already obtained certification**. However, if a suitable repository within these categories cannot be found, the guide provides a comprehensive set of criteria to help researchers evaluate the trustworthiness of other repositories. These criteria, encompassing aspects like **persistent identifiers, metadata quality, data access and usage licenses, and preservation practices**, offer a framework for assessing the reliability and suitability of a repository. 

By encouraging repositories to seek certification, the "Practical Guide" aims to promote a more robust and trustworthy data sharing ecosystem. This benefits researchers by providing them with confidence in the repositories they choose and ultimately contributes to the advancement of open science and the broader research community. 

** Minimum Criteria for Trustworthy Repositories **

Science Europe provide a comprehensive list of criteria that trustworthy repositories should fulfil, particularly in cases where researchers cannot find a suitable discipline-specific or certified repository. These criteria are organised into four main topics:

**1. Provision of Persistent and Unique Identifiers (PIDs):**

*   **Allow data discovery and identification**: The repository should ensure that PIDs, like DOIs, are included in the corresponding metadata to facilitate the identification and discovery of data.
*   **Enable searching, citing, and retrieval of data**: The repository should consistently assign PIDs (e.g., DOI, URN, ARK) to the data, allowing for easy searching, citing, and retrieval, even if the data location changes.
*   **Provide support for data versioning**: The repository should clearly specify and document the version of the data stored, using a permanent audit trail to ensure data provenance can be traced.

It is important to note that not all repositories use universally accepted PID systems. Some rely on local identifiers or administrative numbers maintained by the repository itself, increasing the risk of data loss if the repository undergoes changes or ceases to exist.

**2. Metadata:**

*   **Enable finding of data**: The repository should provide data and metadata in an accessible language based on a well-established formalism to ensure interoperability and reusability. This involves using standard vocabularies and formats that enable computer systems to search for and combine data automatically.
*   **Enable referencing to related relevant information**: The repository should allow linking to other relevant information, such as publications or related datasets, using PIDs and descriptions of the scientific relationship.
*   **Provide information that is publicly available and maintained, even for non-published, protected, retracted, or deleted data**: The repository should ensure the long-term archiving and retrievability of metadata, even if the corresponding research data becomes unavailable due to privacy restrictions, legal obligations, or other protective measures. This also applies to retracted data due to research misconduct, which should remain findable through metadata to allow for examination of the research record.
*   **Use metadata standards that are broadly accepted (by the scientific community)**: The repository should use machine-retrievable metadata and adhere to broadly accepted metadata standards. If the repository specialises in a particular research field, it should follow any existing community standards or best practices for data handling.
*   **Ensure that metadata are machine-retrievable**: The repository should encourage structured metadata that allows for machine retrieval, for example, through forms with specific fields to be completed.

**3. Data Access and Usage Licenses:**

*   **Enable access to data under well-specified conditions**: The repository should clearly specify the terms of data reuse, typically included in the metadata.
*   **Ensure data authenticity and integrity**: The repository should include detailed information about data provenance in the metadata, detailing how the data was generated, processed, and its reliability, to ensure its authenticity and integrity.
*   **Enable retrieval of data**: The repository should allow for data retrieval, or at least metadata retrieval, using open and standardised protocols, avoiding proprietary communication protocols.
*   **Provide information about licensing and permissions (in ideally machine-readable form)**: The repository should present license information in a structured way, ideally understandable by both humans and machines. Using common licensing systems like Creative Commons, which can be referenced by URL, is recommended.
*   **Ensure confidentiality and respect rights of data subjects and creators**: The repository should implement authentication and authorisation procedures for both human and machine users. This allows for user or group-specific access rights, ensuring confidentiality for sensitive data and compliance with data protection regulations.

**4. Preservation:**

*   **Ensure persistence of metadata and data**: The repository should guarantee the long-term preservation and continued availability of the data and metadata entrusted to it.
*   **Be transparent about mission, scope, preservation policies, and plans (including governance, financial sustainability, retention period, and continuity plan)**: The repository should have a documented preservation policy outlining its mission, scope, governance, financial sustainability, retention periods, and procedures for data management.
*   **Have a publicly available contingency plan and ensure preservation of data and metadata beyond the lifetime of the repository**: The repository should have a publicly available contingency plan detailing how data and metadata will be preserved if the repository ceases to exist, for example, through easy extraction and transfer to another repository.

By adhering to these minimum criteria, repositories can demonstrate their trustworthiness and commitment to good data management practices, promoting open science and fostering trust within the research community.

## Links to other DMP templates available online:
[Template for the ERC Open Research Data Management Plan (DMP)](https://erc.europa.eu/sites/default/files/document/file/h2020-erc-tpl-oa-data-mgt-plan_en.docx)

[Horizon 2020 DMP template](https://indico.cern.ch/event/1307393/contributions/5561724/attachments/2715086/4715625/h2020-tpl-oa-data-mgt-plan_en%20Sept%202023.pdf)

**List of examples from Krause et al.**

[C3RDM Template](https://fdm.uni-koeln.de/sites/FDM-UzK/Templates/DMP_UzK_20180201.docx)

[Clarin Wizard (highly recommended)](https://www.clarin-d.net/de/aufbereiten/datenmanagementplan-entwickeln)

[EmiMin](https://github.com/rdmorganiser/rdmo-catalog/tree/master/shared/EmiMin)

[HeFDI](https://github.com/rdmorganiser/rdmo-catalog/tree/master/shared/HeFDI)

[JKI BLE Katalog](https://github.com/rdmorganiser/rdmo-catalog/tree/master/shared/BLE_JKI)

[NFDI4Chem: Hausen, D. A., & Andres, A.-C. (2024). Chemistry-specific Data Management Plan Template based on DFG  checklist. Zenodo. ](https://doi.org/10.5281/zenodo.10948510)

[NFDI4Ing](https://github.com/rdmorganiser/rdmo-catalog/tree/master/shared/nfdi4ing/NFDI4Ing_template)

[MaRDMO Questionaire](https://github.com/MarcoReidelbach/MaRDMO-Questionnaire)

[RDMO DFG-Checkliste](https://github.com/rdmorganiser/rdmo-catalog/blob/master/rdmorganiser/questions/DFG-Checkliste.xml)

[RDMO Horizon Europe](https://github.com/rdmorganiser/rdmo-catalog/blob/master/rdmorganiser/questions/horizon-europe.xml)

[RDMO Katalog](https://github.com/rdmorganiser/rdmo-catalog/blob/master/rdmorganiser/questions/rdmo.xml)

[STAMP](https://www.forschungsdaten-bildung.de/stamp-nutzen)

